---
title: "Data science case for Game Studio Technical Interview"
output:
  rmarkdown::github_document:
    number_sections: true
    toc: true
    toc_depth: 2

editor_options:
  chunk_output_type: console
---

```{r,echo=FALSE,message=FALSE,warning=FALSE}
# Set so that long lines in R will be wrapped:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy=TRUE)
knitr::opts_chunk$set(fig.path = "README_figs/README-")
```

# Case Summary

> You are working as an embedded data scientist for a popular strategy game. Your game director approaches you and says that while sales for the game are meeting KPI targets, some of the engagement KPIs for the game are declining quickly and rapidly. Your assignment is to run an analysis on the data provided to identify the gameplay behaviors that most strongly predict churn. Send us a report with your findings and supporting plots, making recommendations for what the game should implement and any further analysis you think would be valuable.

# Data preparation, preprocessing, and exploration

For both data-sets, I will be: cleaning column names, removing empty columns/rows, and identifying and removing duplicate entries in tables.

```{r packages,warning=FALSE,error=FALSE,message=FALSE,echo=FALSE}
suppressPackageStartupMessages({
library(janitor)
library(lubridate)
library(ggpubr)
library(correlationfunnel)
library(corrplot)
library(MASS)
library(caret)
library(randomForest)
library(party)
library(glmnet)
library(rpart)
library(rpart.plot)
library(tidyverse)})
ggplot2::theme_set(see::theme_modern())

set.seed(1122)
```

## `activity.csv`

Read this table into R, and clean it up a bit.

```{r warning=FALSE,error=FALSE,message=FALSE}
activity<-read.csv("../activity.csv",header=T) %>% clean_names() %>% remove_empty(c("rows", "cols"))
```

Are there duplicate entries for user activity?

```{r warning=FALSE,error=FALSE,message=FALSE}
activity %>% get_dupes(user_id,start_time,final_play) %>% nrow()
```

Yes, there is a large number of duplicate rows! Multiple entries have been included for the same user, with identical `start_time` and `final_play`, only the `total_seconds_played` is different between these entries. To me, this either indicates that there is something wrong with the telemetry tool (this data shouldn't have been included) or there is data missing that would have to distinguish the different entries for a user (i.e. data from separate play sessions).

For now, I have decided to keep a single entry for each user, keeping the one with the most recent information, the greater number of `total_seconds_played`. Otherwise, this duplicate data would present a huge bias!

Update the existing `activity` dataframe.

```{r warning=FALSE,error=FALSE,message=FALSE}
activity<-activity %>%
  group_by(user_id,start_time,final_play) %>% # group by the same variables used to identify the duplicate rows
  slice_max(order_by = total_seconds_played,n = 1,with_ties = FALSE) %>% # only keep the row with the highest `total_seconds_played`, important to set `with_ties = FALSE`.
  ungroup()

activity %>% get_dupes(user_id,start_time,final_play) %>% nrow() # check to make sure cleaning worked...
```

Now that we have cleaned this data and removed duplicate entries, let's take a quick look at some of the trends that appear in preliminary analyses. What does the distribution of play time across these users look like?

```{r warning=FALSE,error=FALSE,message=FALSE}
activity %>%
  mutate(new_to_std=fct_recode(as.factor(new_to_std),"Played Studio's Games Before"="1","Not Played Studio's Games Before"="0")) %>%
  ggplot(aes(x=total_seconds_played,fill=new_to_std))+
  geom_density(alpha=0.5)+
  scale_x_log10()+
  labs(x="Total Seconds Played (log10)",y="Proportion of Players")+
  theme(legend.position = "top",legend.title = element_blank())
```

There is quite a wide range of play times across users. The median here is around 10^4 seconds played.

## `telemetry.csv`

Next, we are importing the `telemetry` data-set and performing some cleaning operations, as I outlined above.

```{r warning=FALSE,error=FALSE,message=FALSE}
telemetry<-read.csv("../telemetry.csv",header=T) %>% clean_names() %>% remove_empty(c("rows", "cols"))

# telemetry %>% get_dupes(user_id) %>% dplyr::select(user_id,dupe_count,total_events) %>% head()
```

There seem to be similar issues with this data as in the `activity` data set. There are multiple entries for each user, events appear to have been logged cumulatively in these users. Although there is one row that contains completely duplicated info. I was thinking that this data may represent user activity during different play sessions, or before/after the tutorial. Yet, this problem isn't exclusive to players that finish the tutorial over these two weeks.

Similar to before, I will retain a single entry for each users based on the higher value of `total_events`.

```{r warning=FALSE,error=FALSE,message=FALSE}
telemetry <- telemetry %>%
  group_by(user_id) %>%
  slice_max(order_by = total_events,n = 1,with_ties = FALSE) %>%
  ungroup()

n_distinct(telemetry$user_id)
```

After cleaning, we get the same number of unique users in both the `activity` and `telemetry` data-sets. This telemetry data has some really interesting variables that could be potentially interesting to explain differences in player behaviours.

## Joining tables

In order to identify users that have churned, we need the columns `start_time` and `final_play` from `activity` as well as the `tutorial_finished` column in `telemetry`. We will need to join these cleaned tables together, by their `user_id`.

```{r warning=FALSE,error=FALSE,message=FALSE}
joined <- right_join(activity,telemetry)
```

This is a weird relationship

```{r}
joined %>% pivot_longer(cols = c(start_game,exit_game)) %>%
  ggplot(aes(x=value,fill=name))+geom_density(alpha=0.5)+scale_x_log10()
```

Further filter by `start_game` and `exit_game`?

```{r}
joined %>% filter(start_game == exit_game)
```

Probably not.

# Predicting Churn

I'm going to construct a couple of models to predict player churn. I evaluate the performance of these models based on the following parameters:

- Precision: Correct positive predictions relative to total positive predictions
- Recall: Correct positive predictions relative to total actual positives
- F1 Score : 2 _ (Precision _ Recall) / (Precision + Recall)

## Defining Churn

Now, back to the topic at hand. We are defining player churn as: 'Players who have not started the game after 2 weeks from their start.'

At first, I interpreted "Players who have not started the game" as "players that have not finished the tutorial" -- this addresses a slightly different question about user retention which I think is interesting, but we don't have the time to address here. For now, I am sticking with the definition of churn as players that still have not logged time in game (`total_seconds_played`==0) 2 weeks after their `start_time`.

The columns `start_time` and `final_play` are key in determining which players have churned. We'll first parse these columns using `lubridate`, take the difference between `start_time` and `final_play` and express this as a number of days. After that, a simple `ifelse` statement assigns a '1' to indicate churn, '0' for players that have not churned.

```{r warning=FALSE,error=FALSE,message=FALSE}
joined.churned<-joined %>%
  column_to_rownames("user_id") %>%
  mutate(start_time=parse_date_time(start_time,order="YmdHMS"),
         final_play=parse_date_time(final_play,order="YmdHMS"),
         difftime=as.period(final_play-start_time,units = "weeks"),
         churn=as.factor(ifelse(difftime>=days(14) & start_game == 0,1,0)))

# joined.churned %>% 
#   dplyr::select(churn,difftime,start_time,final_play,total_seconds_played) %>% 
#   arrange(desc(churn)) %>%
#   head

```

How can a player not have started the game and still have playtime?

Here you can see the assignment of churn: players that who have not started the game after 2 weeks from their start.

```{r warning=FALSE,error=FALSE,message=FALSE}
tabyl(joined.churned,tutorial_finished)
joined.churned %>% filter(tutorial_finished==0) %>% tabyl(.,churn)
```

Only a small percentage of players that have not finished the tutorial have churned.

### Exploratory Analyses

For now, let's look at the distribution of different types of events across users.

```{r warning=FALSE,error=FALSE,message=FALSE,echo=FALSE,fig.cap="Note that in many of these variables are mostly made up of zeros!",fig.align='center'}
joined.churned %>%
  pivot_longer(cols = c(-churn,-start_time,-final_play,-difftime,-new_to_std,-tutorial_finished),names_to = "events") %>% # remove non-integer variables
  ggplot(aes(x=value,fill=events))+
  geom_histogram()+
  scale_x_log10()+
  facet_wrap(~events,scales="free")+
  labs(x="Total Number of Events (Log10)",y="User Count")+
  theme(legend.position = "none",axis.text = element_text(size=6),strip.text=element_text(size=8))
```

### What variables are correlated with churn?

First, I will drop variables that already coincide with how we defined churn.

```{r warning=FALSE,error=FALSE,message=FALSE}
# set variables to move forward with
keep.vars<-joined.churned %>% dplyr::select(-start_time,-final_play,-difftime,-total_seconds_played) %>% colnames()
```

As a first look, I am creating a correlation plot to visualize which variables are positively/negatively correlated with player churn.

```{r warning=FALSE,error=FALSE,message=FALSE}
joined.churned %>%
  dplyr::select(keep.vars) %>%
  mutate(churn=as.numeric(churn)) %>%
  correlate(target = churn) %>%
  filter(feature!="churn") %>%
  plot_correlation_funnel(interactive = FALSE,limits = c(-0.050,0.02))+theme(plot.title = element_blank())
```

In general, these are very weak correlations. Most variables which measure different player events are negatively correlated with player churn. However, players that are new to Studio's player base (`new_to_std`) are positively correlated with churn of that player. The number of `exit_game` events is negatively correlated with churn.

I'm going to drop some variables which are not informative to identifying player behaviours that are associated with churn.

```{r}
keep.vars<-joined.churned %>% dplyr::select(-start_time,-final_play,-difftime,-total_seconds_played,-start_game,-exit_game,-total_events,-decision_events) %>% colnames()
```

### Logistic Regression
Here we will perform logistic regression on a reduced training data set to estimates the probability of churn, based on the independent variables that we have in our joined data set. Split the data into training and testing sets, 80% will go into the training set.

<!-- I also calculated VIF for variables in a first run through this model. While most variables have a moderate VIF value, one in particular was way too high and should be removed from further analyses:`total_events`. -->

```{r warning=FALSE,error=FALSE,message=FALSE}
intrain<- createDataPartition(joined.churned$churn,p=0.8,list=FALSE)
training<- joined.churned[intrain,keep.vars]
# training$churn<-as.numeric(as.character(training$churn))
testing<- joined.churned[-intrain,keep.vars]
# testing$churn<-as.numeric(as.character(testing$churn))
```

Fitting the full model:
```{r warning=FALSE,error=FALSE,message=FALSE,cache=TRUE}
log.fit.all <- glm(churn ~ .,family=binomial(link="logit"),data=training)
print(summary(log.fit.all))
```

Calculate VIF:
```{r,echo=FALSE,eval=FALSE}
car::vif(log.fit.all)
```

Predict:
<!-- Default binomial model the default predictions are of log-odds (probabilities on logit scale) and type = "response" gives the predicted probabilities. -->
```{r warning=FALSE,error=FALSE,message=FALSE,cache=TRUE}
log.pred <- predict(log.fit.all, newdata = testing, type = 'response')

# Convert probs to binary
log.pred <- ifelse(log.pred > 0.5, 1, 0)

# Convert both the predicted and actual values to factors with the same levels
log.pred <- factor(log.pred, levels = c(0, 1))
testing$churn <- factor(testing$churn, levels = c(0, 1))

# Evaluation Metrics
log.result    <- confusionMatrix(data = log.pred, testing$churn)
log.result$byClass['Pos Pred Value']
log.result$byClass['Sensitivity']
log.result$byClass['F1']
```
This is pretty high precision for this model. Perhaps this model is over-fitting?

#### Penialized Logistic Regression

It is likely that not all of these variables are informative in a model to predict player churn. I will perform feature selection using an elastic net regression, which will force some of the less informative variables to be zero, keeping only the ones that are significant.

```{r warning=FALSE,error=FALSE,message=FALSE,cache=TRUE}
x <- model.matrix(churn~., training)[,-1]
# Convert the outcome (class) to a numerical variable
y <- training$churn

# compare measures for cross-validation
cfit <- cv.glmnet(x, y, family = "binomial", keep = TRUE, nlambda = 30)
assess.glmnet(cfit$fit.preval, newy = y, family = "binomial")


# Find the best alpha using cross-validation
cv.lasso <- cv.glmnet(x, y, family = "binomial",type.measure = "auc")
plot(cv.lasso)

# Fit the final model on the training data
lasso <- glmnet(x, y, alpha = 0.8, family = "binomial",lambda = cv.lasso$lambda.min)

# Display regression coefficients
coef(cv.lasso, cv.lasso$lambda.min)
```

Feature selection has kept the following variables:

- `lifestyle_events`
- `help_events`
- `tutorial_finished`

```{r}
# Make predictions on the test data
x.test <- model.matrix(churn ~., testing)[,-1]
lasso.pred <- lasso %>% predict(newx = x.test)
predicted.churn <- ifelse(lasso.pred > 0.5, 1, 0)

# Measure the predictive ability of our logistic regression model, in out testing data-set.
observed.churn <- testing$churn
mean(predicted.churn == observed.churn)
```

After feature selection, the sensitivity of this model is slightly lower.

```{r}
tree <- rpart(as.factor(churn)~lifestyle_events, data = training,method = "class")
rpart.plot(tree)
```

### Construct a Random Forest Model

To compare, I will also construct a Random Forest model using the training data.

```{r warning=FALSE,error=FALSE,message=FALSE,cache=TRUE}
rfModel <- randomForest(as.factor(churn) ~., data = training)
plot(rfModel)
```

Turns out that `rfModel` does not deal with binary factors properly... when we coerce `churn` to be numeric, we get a reasonable result from our RF model.

To comparing the variable importance from the Random Forest model, we look at the Gini scores for each.`MeanDecreaseGini` is an measure of variable importance which shows the average gain of "purity" by the splits of a given variable in a Random Forest model.

```{r warning=FALSE,error=FALSE,message=FALSE}
varImpPlot(rfModel, sort = T, main="Variable Importance")
```

Seems that here `decision_events` and `help_events` are the most important.

#### Random Forest Evaluation metrics

```{r warning=FALSE,error=FALSE,message=FALSE}
forest.pred <- predict(rfModel, newdata = testing, type = "class")

# Ensure both the predicted and actual values are factors with the same levels
forest.pred <- factor(forest.pred, levels = c(0, 1))
testing$churn <- factor(testing$churn, levels = c(0, 1))

# Evaluation Metrics
forest.result <- confusionMatrix(data = forest.pred, reference = testing$churn)

# Output metrics
forest.result$byClass['Pos Pred Value']  # Precision
forest.result$byClass['Sensitivity']     # Recall
forest.result$byClass['F1']              # F1-Score
```

# Summary

## Provided Datasets

There is a large number of duplicate entries in both the `activity` and `telemetry` data-sets. Was there something wrong with how these data-sets were prepared? Or even how they were collected? Either the telemetry fields were not updated, or the `final_play` times were not updated. If this was meant to be more of a continuous data-set, instead of cumulative of a certain period of time, it might be useful in identifying early indicators of churn. If I had more time, I would be really interested in using this type of data to predict indicators of churn. How many players that have finished the tutorial could also be at risk of churning? Is there a temporal nature to the rate of churn (time of year, etc.)?

## Model Performance

Both models seem to be performing extremely well at predicting player churn. This seems a bit suspicious, and could be the result of over-fitting of these models. I removed variables that could be considered co-linear, and performed feature selection, but this still could influence our interpretations of model outcomes. There are slight differences in model performance, with the Random Forest model performing slightly better. If I had more time, I would be able to determine what is causing this over-fitting in out models. Its also possible over/under-sampling could help.

## Indicators of churn

A few variables, `exit_game` and `start_game`, are positively correlated with player churn, but it is unclear how they could be associated with churn. This is a bit odd, but might be indicative of some player behavior that is specific to un-churned players. What is more useful to us are variables which represent in-game actions. The variables `decision_events` and `help_events` were isolated by both models, and they may represent important changes that need to be made in order to retain players for longer and prevent churning in the future. Based on my interpretation of `decision_events`, it may be a good predictor of player churn if churned players make fewer decisions in-game, for the time that they do end up playing for. If new players are overwhelmed with choice, they may be unlikely to continue playing and perceive a game as too complex. Since, only a small proportion of players that haven't finished the tutorial have churned, I would assume that improvements to the tutorial would be necessary in order to make decision events in-game less overwhelming. In addition, `help_events` represent an interesting predictor of churn. Seeing as churned players have fewer `help_events` than un-churned players, it could mean that help messages or material is either harder to find or not as informative as it should be to help new players.

Lastly, we know that the background of the player makes a difference. Players new to Studio's games are more likely to churn compared to players in already in Studio's player base. This makes sense with some of the previous insights that I've made regarding player churn. Interestingly, it's not that these players tend to have spent less time playing in game. So if we can understand what gaps in knowledge these players have, this can inform changes to design/UI to improve the retention of these players, and ultimately make for a better product.

# Extra: Clustering

```{r,echo=FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(janitor)
library(factoextra)
library(FactoMineR)
set.seed(123)
```

## Principal Component Analyses

Using the cleaned, joined datasets, like before in preparation for PCA

```{r,message=FALSE,warning=FALSE}
pca.df<-joined %>%
  select(-start_time,-final_play,-total_events,-start_game,-exit_game) %>%
  column_to_rownames("user_id")
```

- running a PCA with variables scaled to unit variance

```{r,message=FALSE,warning=FALSE}
res.pca <- pca.df %>% 
  PCA(., graph = FALSE)
```

- the amount of variance explained by the first two components is not great...

- showing the eigenvalues of principal components with a scree plot
  
```{r,message=FALSE,warning=FALSE}
fviz_eig(res.pca, addlabels = TRUE)
```

- clearly, only the first two should be retained
- by describing the dimensions, we can identify which variables are most well described by the PCA?

```{r,message=FALSE,warning=FALSE}
res.desc <- dimdesc(res.pca, axes = c(1,2), proba = 0.05)
```

- Description of dimension 1

```{r,message=FALSE,warning=FALSE}
res.desc$Dim.1
```

- multiple variables highly correlated with PC1:
  - `decision_events` 
  - `help_events`
  - `lifestyle_events`

- Description of dimension 2

```{r,message=FALSE,warning=FALSE}
res.desc$Dim.2
```

- only `tutorial_finished` is highly correlated with PC2

- measuring quality and representation
  - `cos2`: represents the quality of representation for variables on the factor map.
  - calculated as the squared coordinates: var.cos2 = var.coord * var.coord.

```{r,message=FALSE,warning=FALSE}
fviz_cos2(res.pca, choice = "var")
```

- the variables that are highly correlated with PC1 are also high quality:
  - `decision_events` 
  - `help_events`
  - `lifestyle_events`

- Measuring the relative contributions of variables to PC1 and PC2
  - dashed line corresponds to the expected value if the contribution were uniform.

```{r}
fviz_contrib(res.pca, choice = "var", axes = 1:2)
```

- several variables contribute more than expected:
  - `decision_events` 
  - `help_events`
  - `lifestyle_events`
  - `tutorial_finished`
  - `newlaw_events`

## K-means Clustering

- Determine the optimal number of clusters to use for k-means.
  - using the "silhouette" method here
    - silouette score: measure of how close each point in one cluster is to points in the neighboring clusters
  - testing through n 2-10
- Taking a subsample here for the sake of saving some time

```{r,message=FALSE,warning=FALSE}
pca.df %>% 
  slice_sample(prop = 0.3) %>% 
  fviz_nbclust(., kmeans, k.max = 10)
```

- Color by membership to clusters groups
- Create a grouping variable using kmeans

```{r,message=FALSE,warning=FALSE}
cntrs = 2
res.km <- kmeans(pca.df, centers = cntrs, nstart = 25)
ind.grp <- as.factor(res.km$cluster)
```

- show PCA with individuals coloured by cluster
- show loadings of variables included in PCA

```{r,message=FALSE,warning=FALSE}
fviz_pca_biplot(res.pca,col.ind = ind.grp, repel = FALSE,label = "var",col.var = "black",addEllipses = T)+guides(color=guide_legend(title="Clusters"),shape="none",fill="none")+theme(legend.position = "top")
```

- here we can clearly see which variables have the most influence on the separation of these clusters:
  - cluster `1`:
    - `tutorial_finished`
    - `total_seconds_played`
    - `suggestion_events`
  - cluster `2`:
    - `decision_events` 
    - `help_events`
    - `lifestyle_events`
    - `newlaw_events`
    - `customization_events`
    - `stress_events`
    
## Clustering Summary:

- a relatively small amount of variance is explained by this PCA
  - ideas for why this is?
  - was normalization/scaling appropriate here?
- clustering has identified the presence of 2 clusters
  - the separation of these clusters is not great
  - I would be curious to see if we can identify additional clusters in this dataset...
- some of the variables that contribute the most, and are the most reliable were also identified as being possible indicators of customer churn.
